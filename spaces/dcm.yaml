params:
  # mixture + representation
  K: {choice: [3, 4, 5, 6]}
  enc_layers: {int: [0, 3]}
  enc_output: {int: [16, 128]}
  enc_hidden: {int: [64, 256]}
  activation:
    choice: ["relu", "gelu"]
  dropout: {uniform: [0.0, 0.30]}
  batchnorm: {choice: [false, true]}
  residual: {choice: [false, true]}
  use_attention: {choice: [false, true]}
  bias: {choice: [true]}

  # VAE (ELBO) terms
  vae_alpha: {loguniform: [1e-3, 1.0]}         # weight on VAE loss
  vae_recon_weight: {loguniform: [0.1, 10.0]}  # decoder likelihood scale

  # gating regularization
  gate_ce_weight: {uniform: [0.05, 0.30]}

  # training
  epochs: {int: [80, 200]}
  batch_size:
    choice: [64, 128, 256]
  lr: {loguniform: [1e-4, 3e-3]}
  weight_decay: {loguniform: [1e-7, 1e-3]}
  patience: {int: [5, 20]}
  val_fraction:
    choice: [0.15]
  lr_scheduler:
    choice: [null, "step", "plateau"]
  interpolation:
    choice: ["step", "linear"]



# Notes:
# For DCM, the set of hyperparameter choices include the number of
# hidden layers for Φ tuned from {1, 2},
# units in each hidden layer selected from {50, 100},
# the number of mixture components K which are tuned between {3, 4, 6}
# and the discounting factor for the VAE-Loss, α tuned from {0, 1} //this is off no vae
# entropy weight can be {0 or 1}
